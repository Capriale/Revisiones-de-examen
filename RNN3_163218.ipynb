{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2cf793b-73fa-4bc9-a980-566516691a68",
   "metadata": {},
   "source": [
    "# Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d0cf63-aeed-4994-9bf5-dee5f0d85d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "esta es una demostración de cómo una rnn puede generar texto basado\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This Python script builds and trains a Recurrent Neural Network (RNN) to generate text based on an input sequence.\n",
    "The model uses an Embedding layer and a SimpleRNN layer to predict the next word in a sequence.\n",
    "\"\"\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Datos de entrada\n",
    "text = \"esta es una demostración de cómo una RNN puede generar texto basado en un texto de entrada.\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Preparar datos\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = []\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = pad_sequences(sequences, maxlen=max(len(seq) for seq in sequences), padding='pre')\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Crear el modelo RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=X.shape[1]))\n",
    "model.add(SimpleRNN(50, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# Generar texto\n",
    "def generate_text(model, tokenizer, seed_text, n_words):\n",
    "    result = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=X.shape[1], padding='pre')\n",
    "        y_pred = np.argmax(model.predict(encoded), axis=-1)\n",
    "        word = tokenizer.index_word[y_pred[0]]\n",
    "        seed_text += ' ' + word\n",
    "        result += ' ' + word\n",
    "    return result\n",
    "\n",
    "# Texto generado\n",
    "print(generate_text(model, tokenizer, 'esta es', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be9a56-20b9-4486-9a17-882aeaf9f279",
   "metadata": {},
   "source": [
    "# Modified Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d5f5fb-907b-44cf-a8ab-7a37ed6d4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "esta es una demostración de cómo una rnn puede generar texto basado\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "\n",
    "# Input data\n",
    "text = \"esta es una demostración de cómo una RNN puede generar texto basado en un texto de entrada.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Prepare data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = []\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[:i+1]\n",
    "    sequences.append(sequence)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Create the modified RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))  # Increased embedding size\n",
    "model.add(Dropout(0.2))  # Dropout after embedding layer\n",
    "model.add(LSTM(50, activation='tanh'))  # Replaced SimpleRNN with LSTM\n",
    "model.add(Dropout(0.2))  # Dropout after LSTM layer\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# Generate text\n",
    "def generate_text(model, tokenizer, seed_text, n_words):\n",
    "    result = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=X.shape[1], padding='pre')\n",
    "        y_pred = np.argmax(model.predict(encoded), axis=-1)\n",
    "        word = tokenizer.index_word.get(y_pred[0], '')\n",
    "        seed_text += ' ' + word\n",
    "        result += ' ' + word\n",
    "    return result\n",
    "\n",
    "# Generated text\n",
    "print(generate_text(model, tokenizer, 'esta es', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754e971-8cac-4bc2-b9f6-b052138d861d",
   "metadata": {},
   "source": [
    "**Explanation of Changes:**\n",
    "\n",
    "* LSTM Layer Impact: Switching to LSTM should improve the model's ability to produce contextually relevant text, as LSTM units are better at managing dependencies over longer sequences than SimpleRNN.\n",
    "* Increased Embedding Dimension: This should improve the richness of word embeddings, allowing the model to better understand relationships between words.\n",
    "* Dropout Layers: Dropout regularizes the model, reducing the chance of overfitting, especially with small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6511b-53c6-47a7-b3dd-431ef5734cbf",
   "metadata": {},
   "source": [
    "**Analysis of Changes and Observations:**\n",
    "\n",
    "The results for both the original and modified codes are nearly identical, indicating that while the model has learned the sequence well, it may be overfitting to the small amount of input text provided. This behavior is expected when training a language model on limited data, as it can quickly memorize the input rather than generalizing patterns for diverse text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
