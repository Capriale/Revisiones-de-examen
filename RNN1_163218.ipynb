{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7ef739-1f9a-459b-b191-9fa72746d493",
   "metadata": {},
   "source": [
    "# Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777f8513-9d09-480d-9099-ac77f11ce046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6264 - loss: 0.6150 - val_accuracy: 0.8068 - val_loss: 0.4366\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.8803 - loss: 0.3103 - val_accuracy: 0.8336 - val_loss: 0.4205\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9323 - loss: 0.1909 - val_accuracy: 0.8144 - val_loss: 0.4516\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9701 - loss: 0.0948 - val_accuracy: 0.8170 - val_loss: 0.4812\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9900 - loss: 0.0409 - val_accuracy: 0.8012 - val_loss: 0.5575\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7945 - loss: 0.5793\n",
      "Test accuracy: 0.7950000166893005\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This Python script builds and trains a Recurrent Neural Network (RNN) to classify the sentiment of\n",
    "movie reviews using the IMDb dataset. The model uses an Embedding layer and a SimpleRNN layer. \n",
    "It is compiled and trained using the TensorFlow and Keras libraries.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Cargar y preprocesar los datos de IMDB\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Crear y entrenar el modelo RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32, input_length=maxlen))\n",
    "model.add(SimpleRNN(32, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f09d09-90f6-4976-82b1-67afc3ae202c",
   "metadata": {},
   "source": [
    "# Modified Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9731208e-60ec-4ed3-a732-a5940b3dca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.6519 - loss: 0.5919 - val_accuracy: 0.8476 - val_loss: 0.3513\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8854 - loss: 0.2874 - val_accuracy: 0.8540 - val_loss: 0.3361\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9152 - loss: 0.2301 - val_accuracy: 0.8438 - val_loss: 0.3663\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.9307 - loss: 0.1930 - val_accuracy: 0.8424 - val_loss: 0.4205\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - accuracy: 0.9455 - loss: 0.1530 - val_accuracy: 0.8332 - val_loss: 0.5057\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8271 - loss: 0.5233\n",
      "Test accuracy: 0.8277999758720398\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "\n",
    "# Load and preprocess the IMDb data\n",
    "max_features = 10000  # Vocabulary size\n",
    "maxlen = 100  # Cut sequences after this many words\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Create and train the modified RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64, input_length=maxlen))  # Increased embedding size\n",
    "model.add(Dropout(0.5))  # Dropout after embedding layer\n",
    "model.add(LSTM(32, activation='tanh'))  # Replaced SimpleRNN with LSTM\n",
    "model.add(Dropout(0.5))  # Dropout after LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec7f39-3eb1-4b64-8f95-93960a15bb11",
   "metadata": {},
   "source": [
    "**Explanation of Changes:**\n",
    "\n",
    "* Embedding size was increased from 32 to 64 to potentially capture more nuanced word relationships.\n",
    "* LSTM layer was used instead of SimpleRNN to help capture long-term dependencies in the data.\n",
    "* Dropout layers were added after the embedding and LSTM layers to improve generalization by reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b66f7e-522e-4c0c-a906-83df21ab102c",
   "metadata": {},
   "source": [
    "The improvement in test accuracy from 0.795 in the original SimpleRNN model to 0.8278 in the modified LSTM model suggests that the changes enhanced the model's ability to capture important features in the IMDb dataset.\n",
    "\n",
    "**Analysis of Changes and Observations:**\n",
    "\n",
    "1. Embedding Size Increase: By raising the embedding dimension, the model could learn more detailed word representations. This improvement allows it to distinguish sentiments more effectively, especially in nuanced cases. The increased embedding dimension likely contributed to the accuracy boost, but it also adds computational cost.\n",
    "\n",
    "2. LSTM Layer Replacement: Switching from SimpleRNN to LSTM has shown to be effective because LSTMs handle long-term dependencies better. For sentiment analysis, context and word dependencies over the review length are crucial, making LSTMs an ideal choice. This change likely provided a more meaningful improvement in classification accuracy.\n",
    "\n",
    "3. Dropout Layers: Adding dropout layers helped reduce overfitting by introducing regularization. By preventing certain neurons from over-relying on specific features during training, dropout forces the network to generalize better. This likely contributed to the higher test accuracy.\n",
    "\n",
    "These adjustments led to a stronger model capable of understanding the underlying sentiment in movie reviews with greater accuracy. The LSTM's ability to maintain information over longer sequences helped capture sentiment context more effectively than the SimpleRNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aab8b6-f50d-4284-b9f0-9a41e208976d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
